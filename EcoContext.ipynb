{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36b633e6-bca3-455b-b555-fd17e9901fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "784be9f3-12c7-40d2-aa79-e40ce840441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceprintExtractor:\n",
    "    def __init__(self, sample_rate=16000, duration=3):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.model = self._build_feature_extractor()\n",
    "        \n",
    "    def _build_feature_extractor(self):\n",
    "        inputs = Input(shape=(128, 40))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        outputs = Dense(32, activation=None)(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def extract_features(self, audio_data):\n",
    "        try:\n",
    "            mfccs = librosa.feature.mfcc(y=audio_data, \n",
    "                                        sr=self.sample_rate,\n",
    "                                        n_mfcc=40)\n",
    "            mfccs = mfccs.T\n",
    "            if mfccs.shape[0] < 128:\n",
    "                mfccs = np.pad(mfccs, ((0, 128 - mfccs.shape[0]), (0, 0)))\n",
    "            else:\n",
    "                mfccs = mfccs[:128, :]\n",
    "            return mfccs\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_voiceprint(self, audio_data):\n",
    "        features = self.extract_features(audio_data)\n",
    "        if features is None:\n",
    "            return None\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "        embedding = self.model.predict(features, verbose=0)\n",
    "        return embedding.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51a96693-9b1c-4f36-9ea9-b05899c85040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwareness:\n",
    "    def __init__(self):\n",
    "        self.fuzzy_system = self._build_fuzzy_system()\n",
    "        \n",
    "    def _build_fuzzy_system(self):\n",
    "        noise_level = ctrl.Antecedent(np.arange(0, 101, 1), 'noise_level')\n",
    "        movement = ctrl.Antecedent(np.arange(0, 101, 1), 'movement')\n",
    "        sensitivity = ctrl.Consequent(np.arange(0, 101, 1), 'sensitivity')\n",
    "        power_mode = ctrl.Consequent(np.arange(0, 101, 1), 'power_mode')\n",
    "        \n",
    "        noise_level['low'] = fuzz.trimf(noise_level.universe, [0, 0, 40])\n",
    "        noise_level['medium'] = fuzz.trimf(noise_level.universe, [20, 50, 80])\n",
    "        noise_level['high'] = fuzz.trimf(noise_level.universe, [60, 100, 100])\n",
    "        \n",
    "        movement['stationary'] = fuzz.trimf(movement.universe, [0, 0, 30])\n",
    "        movement['walking'] = fuzz.trimf(movement.universe, [20, 50, 80])\n",
    "        movement['running'] = fuzz.trimf(movement.universe, [70, 100, 100])\n",
    "    \n",
    "        sensitivity['low'] = fuzz.trimf(sensitivity.universe, [0, 0, 40])\n",
    "        sensitivity['medium'] = fuzz.trimf(sensitivity.universe, [30, 50, 70])\n",
    "        sensitivity['high'] = fuzz.trimf(sensitivity.universe, [60, 100, 100])\n",
    "        \n",
    "        power_mode['eco'] = fuzz.trimf(power_mode.universe, [0, 0, 40])\n",
    "        power_mode['balanced'] = fuzz.trimf(power_mode.universe, [30, 50, 70])\n",
    "        power_mode['performance'] = fuzz.trimf(power_mode.universe, [60, 100, 100])\n",
    "    \n",
    "        rule1 = ctrl.Rule(noise_level['high'], sensitivity['high'])\n",
    "        rule2 = ctrl.Rule(noise_level['low'], sensitivity['low'])\n",
    "        rule3 = ctrl.Rule(movement['running'], power_mode['performance'])\n",
    "        rule4 = ctrl.Rule(movement['stationary'], power_mode['eco'])\n",
    "        rule5 = ctrl.Rule(movement['walking'] & noise_level['medium'], \n",
    "                         (sensitivity['medium'], power_mode['balanced']))\n",
    "\n",
    "        system = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5])\n",
    "        return ctrl.ControlSystemSimulation(system)\n",
    "    \n",
    "    def detect_noise_level(self, audio_data):\n",
    "        if len(audio_data) > 0:\n",
    "            energy = np.sqrt(np.mean(np.square(audio_data)))\n",
    "            noise_level = min(100, max(0, energy * 100))\n",
    "            return noise_level\n",
    "        return 0\n",
    "    \n",
    "    def detect_movement(self, acceleration_data=None):\n",
    "        if acceleration_data is None:\n",
    "            return np.random.uniform(0, 100)\n",
    "        else:\n",
    "            movement = np.mean(np.abs(acceleration_data))\n",
    "            return min(100, max(0, movement * 10))\n",
    "            \n",
    "    def adapt_to_context(self, noise_level, movement):\n",
    "        try:\n",
    "            self.fuzzy_system.input['noise_level'] = noise_level\n",
    "            self.fuzzy_system.input['movement'] = movement\n",
    "    \n",
    "            self.fuzzy_system.compute()\n",
    "            sensitivity = self.fuzzy_system.output['sensitivity']\n",
    "            power_mode = self.fuzzy_system.output['power_mode']\n",
    "            return {\n",
    "                'sensitivity': sensitivity,\n",
    "                'power_mode': power_mode\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fuzzy adaptation: {e}\")\n",
    "            return {\n",
    "                'sensitivity': 50, \n",
    "                'power_mode': 50\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d40fee1-85e5-4119-9db7-ea8400c88468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyOptimizer:\n",
    "    def __init__(self):\n",
    "        self.power_states = {\n",
    "            'sleep': 5,      \n",
    "            'listening': 14, \n",
    "            'processing': 25\n",
    "        }\n",
    "        self.current_state = 'sleep'\n",
    "        self.current_power = self.power_states['sleep']\n",
    "        \n",
    "    def optimize_power(self, context_data):\n",
    "        power_mode = context_data.get('power_mode', 50)\n",
    "        if power_mode < 30:\n",
    "            self.set_power_state('sleep')\n",
    "        elif power_mode < 70:\n",
    "            self.set_power_state('listening')\n",
    "        else:\n",
    "            self.set_power_state('processing')\n",
    "        return self.current_power\n",
    "        \n",
    "    def set_power_state(self, state):\n",
    "        if state in self.power_states:\n",
    "            self.current_state = state\n",
    "            self.current_power = self.power_states[state]\n",
    "            print(f\"Power state changed to {state} ({self.current_power} mW)\")\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def estimate_battery_life(self, battery_capacity_mAh=1000, voltage=3.7):\n",
    "        battery_capacity_mWh = battery_capacity_mAh * voltage\n",
    "        estimated_hours = battery_capacity_mWh / self.current_power\n",
    "        return estimated_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "316e1d73-236d-450d-a16a-d612865db4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcoContextVoxID:\n",
    "    def __init__(self):\n",
    "        self.voiceprint_extractor = VoiceprintExtractor()\n",
    "        self.context_awareness = ContextAwareness()\n",
    "        self.energy_optimizer = EnergyOptimizer()\n",
    "        self.users = {} \n",
    "        self.current_user = None\n",
    "        \n",
    "    def register_user(self, user_id, audio_data):\n",
    "        voiceprint = self.voiceprint_extractor.get_voiceprint(audio_data)\n",
    "        if voiceprint is not None:\n",
    "            self.users[user_id] = voiceprint\n",
    "            print(f\"User {user_id} registered successfully\")\n",
    "            return True\n",
    "        print(f\"Failed to register user {user_id}\")\n",
    "        return False\n",
    "        \n",
    "    def identify_user(self, audio_data):\n",
    "        if not self.users:\n",
    "            print(\"No users registered\")\n",
    "            return None\n",
    "            \n",
    "        test_voiceprint = self.voiceprint_extractor.get_voiceprint(audio_data)\n",
    "        if test_voiceprint is None:\n",
    "            return None\n",
    "        best_match = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for user_id, voiceprint in self.users.items():\n",
    "            similarity = self._cosine_similarity(test_voiceprint, voiceprint)\n",
    "            if similarity > best_score:\n",
    "                best_score = similarity\n",
    "                best_match = user_id\n",
    "        \n",
    "        if best_score > 0.85:\n",
    "            self.current_user = best_match\n",
    "            return best_match\n",
    "        return None\n",
    "        \n",
    "    def _cosine_similarity(self, a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        \n",
    "    def process_context(self, audio_data, acceleration_data=None):\n",
    "        noise_level = self.context_awareness.detect_noise_level(audio_data)\n",
    "        movement = self.context_awareness.detect_movement(acceleration_data)\n",
    "        context_data = self.context_awareness.adapt_to_context(noise_level, movement)\n",
    "        self.energy_optimizer.optimize_power(context_data)\n",
    "        return context_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6329c4-f032-43f0-ac59-81518cbdbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_audio(duration=3, sample_rate=16000, noise_level=0.1):\n",
    "    return np.random.normal(0, noise_level, int(duration * sample_rate))\n",
    "\n",
    "vox_system = EcoContextVoxID()\n",
    "\n",
    "print(\"Creating simulated audio data for users...\")\n",
    "user1_samples = [generate_fake_audio(noise_level=0.1) for _ in range(3)]\n",
    "user2_samples = [generate_fake_audio(noise_level=0.15) for _ in range(3)]\n",
    "\n",
    "# %%\n",
    "# Register users\n",
    "print(\"\\nRegistering users...\")\n",
    "vox_system.register_user(\"Alice\", user1_samples[0])\n",
    "vox_system.register_user(\"Bob\", user2_samples[0])\n",
    "\n",
    "# %%\n",
    "# Test user identification\n",
    "print(\"\\nTesting user identification...\")\n",
    "test_audio = user1_samples[1]  # Use a different sample from the same user\n",
    "identified_user = vox_system.identify_user(test_audio)\n",
    "print(f\"Identified user: {identified_user}\")\n",
    "\n",
    "# %%\n",
    "# Demonstrate context adaptation\n",
    "print(\"\\nDemonstrating context adaptation...\")\n",
    "test_contexts = [\n",
    "    {\"name\": \"Quiet home\", \"noise\": 0.05, \"movement\": 5},\n",
    "    {\"name\": \"Noisy street\", \"noise\": 0.5, \"movement\": 40},\n",
    "    {\"name\": \"Running outdoors\", \"noise\": 0.2, \"movement\": 90}\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "power_values = []\n",
    "sensitivity_values = []\n",
    "\n",
    "for i, context in enumerate(test_contexts):\n",
    "    print(f\"\\nSimulating context: {context['name']}\")\n",
    "    # Generate fake audio with appropriate noise level\n",
    "    audio = generate_fake_audio(noise_level=context[\"noise\"])\n",
    "    \n",
    "    # Process the context\n",
    "    context_result = vox_system.process_context(\n",
    "        audio,\n",
    "        np.array([context[\"movement\"]/10])\n",
    "    )\n",
    "    \n",
    "    print(f\"Sensitivity level: {context_result['sensitivity']:.1f}/100\")\n",
    "    print(f\"Power mode: {context_result['power_mode']:.1f}/100\")\n",
    "    print(f\"Current power usage: {vox_system.energy_optimizer.current_power} mW\")\n",
    "    \n",
    "    # Simulate battery life\n",
    "    battery_life = vox_system.energy_optimizer.estimate_battery_life()\n",
    "    print(f\"Estimated battery life: {battery_life:.1f} hours\")\n",
    "    \n",
    "    power_values.append(vox_system.energy_optimizer.current_power)\n",
    "    sensitivity_values.append(context_result['sensitivity'])\n",
    "\n",
    "# %%\n",
    "# Plot results\n",
    "contexts = [c[\"name\"] for c in test_contexts]\n",
    "    \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(contexts, power_values)\n",
    "plt.title('Power Consumption by Context')\n",
    "plt.ylabel('Power (mW)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(contexts, sensitivity_values)\n",
    "plt.title('Sensitivity Level by Context')\n",
    "plt.ylabel('Sensitivity Level')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSystem demo completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
